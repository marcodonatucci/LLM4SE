{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# set seed \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore the core components of the Transformer model, which has revolutionized natural language processing and sequence modeling. The Transformer model, introduced by Vaswani et al. in 2017 (https://arxiv.org/pdf/1706.03762).\n",
    "\n",
    "By the end of this exercise, you will have a solid grasp of how each part of the Transformer contributes to its success, and how attention mechanisms allow it to focus on relevant parts of an input sequence when making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transformer_encoder.png](images/transformer_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Synthetic Time Series Dataset with Jumps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define a **custom dataset class** that generates synthetic time series data with jumps at regular intervals. This class, `TimeSeriesDataset`, is designed to simulate a flat time series with pseudorandom spikes introduced at specific points. \n",
    "Each time series begins as a flat line and has a jump introduced at a random point within the first few time steps. Additional jumps occur at regular intervals, alternating between small and large jumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Creating a Synthetic Time Series Dataset with Jumps\n",
    "\n",
    "In this exercise, you will implement a class to create a synthetic time series dataset. The dataset consists of flat sequences with periodic jumps, simulating data where the value remains constant for a while before increasing significantly at regular intervals. \n",
    "\n",
    "#### Instructions:\n",
    "- Implement the `TimeSeriesDataset` class that inherits from `torch.utils.data.Dataset`.\n",
    "- Each time series should have a predefined number of jumps, with the jumps occurring every `val = 3` timesteps.\n",
    "- The first jump should occur at a random point between `0` and `val`, with the size of the jump determined by `initial_jump = 5` plus some random noise.\n",
    "- After the first jump, subsequent jumps occur every `val` timesteps, alternating between no jump (`0`) and a full jump (`initial_jump`), again with some random noise.\n",
    "- The dataset should return the time series and the value of the final jump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the TimeSeriesDataset class to generate a dataset of time series with jumps\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_len, num_samples):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by generating a number of time series samples with jumps.\n",
    "\n",
    "        Args:\n",
    "        - seq_len (int): The length of each time series sequence.\n",
    "        - num_samples (int): The number of samples in the dataset.\n",
    "        \n",
    "        Notes:\n",
    "        - Each time series will have jumps every 'val' steps, starting at a random point.\n",
    "        - The size of the jump is 'initial_jump' plus some random noise.\n",
    "        \"\"\"\n",
    "        val = 3  \n",
    "        initial_jump = 5 \n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            time_series = np.zeros(seq_len + 1)\n",
    "            \n",
    "            # TODO: Introduce the first jump at a random point between 0 and val\n",
    "            first_jump = ...\n",
    "            time_series[first_jump] += ...  # Add a jump with some random noise\n",
    "            \n",
    "            # TODO: Continue introducing jumps every val timesteps after the first jump\n",
    "            for i in range(first_jump + val, seq_len + 1, val):\n",
    "                jump = 0 if i % 2 == 1 else initial_jump  # Alternate between 0 and initial_jump\n",
    "                time_series[i] += ...  # Add a jump with some random noise\n",
    "            \n",
    "            self.data.append(time_series)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # TODO: Return the length of the dataset\n",
    "        return ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the time series at the specified index.\n",
    "        \n",
    "        Args:\n",
    "        - idx (int): The index of the desired time series.\n",
    "        \n",
    "        Returns:\n",
    "        - A tuple of (time series, final jump value)\n",
    "        \"\"\"\n",
    "        series = ...\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample of the dataset\n",
    "\n",
    "dataset = TimeSeriesDataset(seq_len=40, num_samples=1)\n",
    "x, y = dataset[0]\n",
    "plt.plot(range(40), x, label='x')\n",
    "plt.title(f\"Example Time Series Data Point\\n y: {y}\")\n",
    "plt.scatter(40, y, color='r', label='y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting sample should look like this:\n",
    "\n",
    "![sample_dataset.png](images/sample_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism allows the model to weigh the importance of different parts of the input sequence relative to each other. In the **Self-Attention** mechanism, each word in the input is compared to every other word to determine their relevance.\n",
    "\n",
    "The attention mechanism is mathematically defined as:\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "Where:\n",
    "- $ Q $ is the query matrix\n",
    "- $ K $ is the key matrix\n",
    "- $ V $ is the value matrix\n",
    "- $ d_k $ is the dimensionality of the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism operates in the following steps:\n",
    "\n",
    "- **Score Calculation**: The attention scores are calculated by taking the dot product between the query and the transposed key matrices. This operation measures how relevant each position in the key is to the query. The scores are scaled by the square root of the dimensionality of the key (`d_k`) to avoid large gradients, which could destabilize the training process.\n",
    "\n",
    "- **Relative Positional Encoding**: In addition to the attention scores, we add learnable **relative positional encodings** to capture the relationships between elements in the sequence. This encoding helps the model understand the relative positions of tokens without relying solely on absolute positional information. The relative encodings are stored in a learnable layer, which is added to the attention scores before applying softmax. If another type of positional encoding is present, this step is skipped.\n",
    "\n",
    "- **Masking (Optional)**: If a mask is provided, it is applied to the scores by setting specific elements to a very large negative value. This prevents the model from attending to certain positions, which is particularly useful when handling padded sequences in time series or natural language tasks. For this particular problem the mask is not required, so it is not present in the implementation.\n",
    "\n",
    "- **Softmax**: After computing the scaled scores, the softmax function is applied to normalize the scores into probabilities. This step ensures that the attention weights sum to 1 across each sequence position, allowing the model to attend to specific parts of the input more heavily.\n",
    "\n",
    "- **Attention Output**: Finally, the attention weights are used to compute a weighted sum of the value matrix, generating the output. This allows the model to focus on important information in the sequence and ignore less relevant parts.\n",
    "\n",
    "The attention mechanism returns both the final output and the attention weights, providing insight into which parts of the input the model is focusing on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Scaled Dot-Product Attention Mechanism\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, seq_len, d_k, other_positional_encodings_present):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.seq_len = seq_len\n",
    "        self.other_positional_encodings_present = other_positional_encodings_present\n",
    "\n",
    "        self.rel_layer = torch.randn(1, self.seq_len, self.seq_len, requires_grad=True) # Create a learnable relative positional encoding\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Compute the attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) # Scaled Dot-Product\n",
    "        \n",
    "        # Add relative positional encodings if no other positional encodings are present\n",
    "        if not self.other_positional_encodings_present:\n",
    "            scores = scores + self.rel_layer\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention head encapsulates the `ScaledDotProductAttention` mechanism by preparing the query, key, and value through linear layers. After computing attention, the result is passed through a final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implementing an Attention Head\n",
    "\n",
    "In this exercise, you will implement the **AttentionHead** class, which is responsible for applying the attention mechanism to the input sequence. The attention head combines linear transformations and the **Scaled Dot-Product Attention** mechanism to allow the model to focus on relevant parts of the sequence.\n",
    "\n",
    "#### Instructions:\n",
    "- Implement the `AttentionHead` class that inherits from `torch.nn.Module`.\n",
    "- Define three linear layers for the **query**, **key**, and **value** matrices, which will project the input into separate subspaces.\n",
    "- Pass the query, key, and value matrices through the **Scaled Dot-Product Attention** mechanism to compute the attention output and weights.\n",
    "- Apply a final linear layer to the attention output to produce the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, other_positional_encodings_present):\n",
    "        \"\"\"\n",
    "        Initializes the AttentionHead.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): The dimensionality of the input and output features.\n",
    "        - seq_len (int): The length of the input sequence.\n",
    "        - other_positional_encodings_present (bool): Whether other positional encodings are being used.\n",
    "        \n",
    "        Notes:\n",
    "        - This class uses the Scaled Dot-Product Attention mechanism to compute attention.\n",
    "        - It also applies linear transformations to the query, key, and value matrices.\n",
    "        \"\"\"\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.d_k = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # TODO: Define linear layers for query, key, and value\n",
    "        self.query_layer = ...\n",
    "        self.key_layer = ...\n",
    "        self.value_layer = ...\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention mechanism\n",
    "        self.scaled_dot_attention = ScaledDotProductAttention(self.seq_len, self.d_k, other_positional_encodings_present)\n",
    "\n",
    "        # TODO: Define the final output linear layer\n",
    "        self.fc_out = ...\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Computes the attention output and weights.\n",
    "\n",
    "        Args:\n",
    "        - query: The query matrix (shape: [batch_size, seq_len, d_model]).\n",
    "        - key: The key matrix (shape: [batch_size, seq_len, d_model]).\n",
    "        - value: The value matrix (shape: [batch_size, seq_len, d_model]).\n",
    "        \n",
    "        Returns:\n",
    "        - output: The final output after applying attention.\n",
    "        - attention_weights: The attention weights for each query/key pair.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Apply the linear transformations to the query, key, and value matrices\n",
    "        Q = ...\n",
    "        K = ...\n",
    "        V = ...\n",
    "\n",
    "        # TODO: Apply the scaled dot-product attention mechanism\n",
    "        attention_output, attention_weights = ...\n",
    "\n",
    "        # TODO: Squeeze the attention output and pass it through the final linear layer\n",
    "        attention_output = ...\n",
    "        output = ...\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Transformer Encoder Block** is a fundamental building block of the Transformer architecture, used to process input sequences by combining attention mechanisms and feed-forward networks. The block consists of two primary sublayers:\n",
    "\n",
    "1. **Attention Head**: The input sequence is passed through the **AttentionHead**, which computes the scaled dot-product attention by projecting the query, key, and value matrices using linear layers. The attention output is then combined with the original input through a residual connection and normalized using **LayerNorm**.\n",
    "\n",
    "2. **Feed-Forward Network (FFN)**: After the attention sublayer, the normalized output is passed through a feed-forward network consisting of two linear layers with a ReLU activation function. The first linear layer expands the dimensionality of the input, and the second linear layer projects it back to the original dimension.\n",
    "\n",
    "By stacking multiple encoder blocks, the Transformer model is able to build hierarchical representations of the input, allowing it to capture complex relationships and patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic Transformer Encoder block\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, seq_len, other_positional_encodings_present, dropout=0.1):\n",
    "        '''\n",
    "        Initializes a basic Transformer Encoder block.\n",
    "\n",
    "        Args:\n",
    "        - d_model (int): The dimensionality of the input and output features.\n",
    "        - dim_feedforward (int): The dimensionality of the feedforward network.\n",
    "        - seq_len (int): The length of the input sequence.\n",
    "        - other_positional_encodings_present (bool): Whether other positional encodings are being used.\n",
    "        - dropout (float): The dropout probability.\n",
    "\n",
    "        Notes:\n",
    "        - This class uses the AttentionHead defined above.\n",
    "        '''\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = AttentionHead(d_model, seq_len, other_positional_encodings_present)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        #TODO: Define the second layer normalization\n",
    "        self.norm2 = ...\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes the input through the Transformer Encoder block.\n",
    "\n",
    "        Args:\n",
    "        - x: The input tensor (shape: [batch_size, seq_len, d_model]).\n",
    "\n",
    "        Returns:\n",
    "        - output: The output tensor (shape: [batch_size, seq_len, d_model]).\n",
    "        - attention_weights: The attention weights for each query/key pair.\n",
    "        '''\n",
    "        \n",
    "        # Attention head\n",
    "        attention_output, attention_weights = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "\n",
    "        # TODO: Pass the output through the feed-forward network\n",
    "        ffn_output = ...\n",
    "\n",
    "        #TODO: Apply the second layer normalization with the residual connection\n",
    "        output = ...\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the **Transformer Model** designed for time series data is implemented. Below are the key components of the model:\n",
    "\n",
    "1. **Embedding Layer**: The input time series is first passed through a linear embedding layer that projects the input features into a higher-dimensional space (`d_model`). This transformation enables the model to work with rich feature representations of the input data.\n",
    "\n",
    "2. **Positional Encoding**: Since the Transformer does not inherently capture the order of the input sequence, positional encodings are added to provide information about the position of each time step in the sequence. If a positional encoding method is provided, it is applied to the embedded input. Otherwise, the **Relative Positional Encoding** provided in `ScaledDotProductAttention` is applied.\n",
    "\n",
    "3. **Transformer Encoder Layers**: The model contains `num_layers` **TransformerEncoderBlocks**, which apply both attention mechanisms and feed-forward networks to the input sequence. These encoder layers are stacked to progressively build hierarchical representations of the input data. Each layer outputs attention weights, which provide insight into which parts of the sequence the model is focusing on.\n",
    "\n",
    "4. **Output Layer**: After processing the input through the encoder layers, the model applies a final linear layer to predict the output. For time series regression tasks, the model uses the output from the last time step in the sequence to generate a single value.\n",
    "\n",
    "Throughout the forward pass, attention weights from each encoder layer are collected. These weights can be used for interpretability, showing how the model distributes its focus across the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, dim_feedforward, num_layers, seq_len, positional_encoding = None, dropout=0.1, max_len=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "\n",
    "        # Embedding for the time series input\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding for time series data\n",
    "        if self.positional_encoding is not None:\n",
    "            self.pos_encoder = self.positional_encoding(d_model, max_len)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(d_model, dim_feedforward, seq_len, False if positional_encoding is None else True, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # Output layer to predict a single value\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_size]\n",
    "        \n",
    "        # Embed the input time series data\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_len, d_model]    \n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x) if self.positional_encoding is not None else x\n",
    "        \n",
    "        # List to store attention weights from all layers\n",
    "        all_attention_weights = []\n",
    "\n",
    "        # Pass through each Transformer encoder layer\n",
    "        for layer in self.encoder_layers:\n",
    "            x, attention_weights = layer(x)\n",
    "            all_attention_weights.append(attention_weights)\n",
    "\n",
    "        # Apply final linear layer to get the output\n",
    "        output = self.fc_out(x[:, -1])  # Take the output of the last time step (sequence regression)\n",
    "        return output, all_attention_weights  # Return the final output and the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1  # Number of input features\n",
    "seq_len = 40 # Length of the time series sequence\n",
    "\n",
    "d_model = 64  # Embedding dimension\n",
    "dim_feedforward = 256  # Feedforward network dimension\n",
    "num_layers = 3  # Number of Transformer encoder layers\n",
    "\n",
    "model = Transformer(input_size, d_model, dim_feedforward, num_layers, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Trains the given model on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to be trained.\n",
    "    - dataloader: A DataLoader object to iterate over the dataset.\n",
    "    - criterion: The loss function used to compute the training loss.\n",
    "    - optimizer: The optimizer used to update the model parameters.\n",
    "    - num_epochs: The number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "    - loss_history: A list containing the loss value for each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Initialize an empty list to store the loss history\n",
    "    loss_history = []\n",
    "    \n",
    "    # TODO: Loop through the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # TODO: Iterate over the batches of the dataloader\n",
    "        for inputs, target in dataloader:\n",
    "            # TODO: Add a feature dimension to the input data (input_size is assumed to be 1)\n",
    "            inputs = ...  # Shape: [batch_size, seq_len, input_size]\n",
    "            \n",
    "            # TODO: Zero the gradients of the optimizer\n",
    "            \n",
    "            \n",
    "            # TODO: Perform a forward pass through the model and compute the loss\n",
    "            outputs, _ = ...\n",
    "            loss = ...\n",
    "            \n",
    "            # TODO: Perform backpropagation to compute the gradients\n",
    "            \n",
    "            \n",
    "            # TODO: Update the model's parameters using the optimizer\n",
    "            \n",
    "            \n",
    "            # Accumulate the total loss for this batch\n",
    "            total_loss += ...\n",
    "        \n",
    "        # TODO: Compute the average loss for the current epoch\n",
    "        avg_loss = ...\n",
    "    \n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}')\n",
    "    \n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss During Training')\n",
    "    plt.show()\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # Number of samples in each batch\n",
    "num_samples = 1000 # Number of samples in the dataset\n",
    "num_epochs = 15 # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the DataLoader\n",
    "dataset = ...\n",
    "dataloader = ...\n",
    "\n",
    "# TODO: Instantiate the criterion (MSE loss) and the optimizer (Adam) with a learning rate of 0.0005\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Normalized Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot displays the **Input Series** (blue line) along with the corresponding **Normalized Attention Weights** (orange dashed line). The normalized attention weights highlight the time steps that the Transformer model focuses on during the prediction process.\n",
    "\n",
    "What's interesting is how the model places more attention on previous jumps in the sequence. These small spikes in the attention weights indicate that the model is focusing on key points in the input that help it make a more accurate prediction. The red dot marks the **true target value** (5.51), and the green dot shows the **predicted value** (5.63). The fact that the predicted value is so close to the true target demonstrates how well the model is learning from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Visualize the Attention Weights\n",
    "def visualize_attention(model, dataset):\n",
    "    model.eval()\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, target = dataset[i]\n",
    "        if target != 0: \n",
    "            # Reshape the input for the Transformer: [batch_size, seq_len, input_size]\n",
    "            inputs = inputs.unsqueeze(0).unsqueeze(2)\n",
    "            outputs, attention_weights = model(inputs)\n",
    "            break\n",
    "\n",
    "    # Plot the input time series (after undoing the batch and input dimension)\n",
    "    plt.plot(inputs.permute(1, 0, 2).squeeze().numpy(), label=\"Input Series\")\n",
    "\n",
    "    # The attention weights are returned from all layers, so let's use the attention from the last layer\n",
    "    attention_weights = attention_weights[-1][0].detach().numpy()  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "    # Normalize attention weights for better visibility\n",
    "    normalized_attention_weights = (attention_weights - np.min(attention_weights)) / (np.max(attention_weights) - np.min(attention_weights))\n",
    "    \n",
    "    \n",
    "    # Plot the normalized attention weights\n",
    "    plt.plot(normalized_attention_weights.mean(axis=0), label=\"Normalized Attention Weights\", linestyle='--')\n",
    "    plt.scatter(seq_len, target, color='r', label='True Target')\n",
    "    plt.scatter(seq_len, outputs[-1].item(), color='g', label='Predicted Target')\n",
    "    plt.title(f\"Example of a prediction:\\nTrue Target: {target.item():.2f}, Predicted: {outputs[-1].item():.2f}\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_attention(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Relative Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standard Transformer architectures, **absolute positional encodings** are commonly used to inject information about the position of each token or time step into the model. This works well for tasks where the exact position of each element in the sequence is critical. However, for many time series and sequence-based datasets, the **relative position** between elements often carries more significance than their absolute position. This is where **relative positional encoding** becomes crucial.\n",
    "\n",
    "In this model, **relative positional encoding** is applied due to the specific nature of the dataset, where the height of each jump depends on the previous one, and jumps occur consistently every `val = 3` timesteps. The dataset contains strong temporal dependencies, making it vital for the model to assess the relative distance between events, regardless of their absolute position. Additionally, since the jumps are shifted between the different time series in the dataset, the model must capture these relationships in a way that generalizes across varying time series sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Between Absolute and Relative Positional Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **PositionalEncoding** class shown below implements absolute positional encoding, where each position in the sequence is encoded using a combination of sine and cosine functions. This encoding is then added to the input embeddings to provide the model with information about the position of each token in the sequence. Here’s how the code works:\n",
    "\n",
    "- **Matrix Creation**: A positional encoding matrix `pe` of shape `(max_len, d_model)` is initialized, where `max_len` is the maximum sequence length and `d_model` is the dimensionality of the model.\n",
    "- **Position Calculation**: The `position` tensor is created, holding the position values for each token in the sequence. The `div_term` tensor calculates the scaling factor for the sine and cosine functions based on the position and the model’s dimensionality.\n",
    "- **Sine and Cosine Application**: The sine function is applied to the even indices, and the cosine function is applied to the odd indices of the positional encoding matrix. This ensures that each position has a unique encoding based on a combination of sine and cosine functions.\n",
    "- **Buffer Registration**: The positional encoding matrix `pe` is registered as a buffer, meaning it is a fixed parameter that does not get updated during backpropagation.\n",
    "- **Forward Pass**: During the forward pass, the positional encoding is added to the input, giving the model information about the absolute positions of each token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute sine and cosine positional encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices in the encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices in the encoding\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Reshape for batch compatibility\n",
    "        self.register_buffer('pe', pe)  # Register pe as a buffer so it doesn't get updated during backpropagation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to the input\n",
    "        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize the Transformer model with the PositionalEncoding class\n",
    "model = ...\n",
    "\n",
    "# TODO: Instantiate the criterion (MSE loss) and the optimizer (Adam) with a learning rate of 0.0005\n",
    "criterion = ...\n",
    "optimizer = ...\n",
    "\n",
    "# TODO: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Visualize the Attention Weights\n",
    "def visualize_attention(model, dataset):\n",
    "    model.eval()\n",
    "    for i in range(len(dataset)):\n",
    "        inputs, target = dataset[i]\n",
    "        if target != 0: \n",
    "            # Reshape the input for the Transformer: [batch_size, seq_len, input_size]\n",
    "            inputs = inputs.unsqueeze(0).unsqueeze(2)\n",
    "            outputs, attention_weights = model(inputs)\n",
    "            break\n",
    "\n",
    "    # Plot the input time series (after undoing the batch and input dimension)\n",
    "    plt.plot(inputs.permute(1, 0, 2).squeeze().numpy(), label=\"Input Series\")\n",
    "\n",
    "    # The attention weights are returned from all layers, so let's use the attention from the last layer\n",
    "    attention_weights = attention_weights[-1][0].detach().numpy()  # [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "    # Normalize attention weights for better visibility\n",
    "    normalized_attention_weights = (attention_weights - np.min(attention_weights)) / (np.max(attention_weights) - np.min(attention_weights))\n",
    "    \n",
    "    \n",
    "    # Plot the normalized attention weights\n",
    "    plt.plot(normalized_attention_weights.mean(axis=0), label=\"Normalized Attention Weights\", linestyle='--')\n",
    "    plt.scatter(seq_len, target, color='r', label='True Target')\n",
    "    plt.scatter(seq_len, outputs[-1].item(), color='g', label='Predicted Target')\n",
    "    plt.title(f\"Example of a prediction:\\nTrue Target: {target.item():.2f}, Predicted: {outputs[-1].item():.2f}\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "visualize_attention(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key observation from this experiment is that the model using absolute positional encoding performed significantly worse than the one using relative positional encoding. This performance difference highlights the fact that, in this particular dataset, the **relative position** between events is much more important than their absolute position. Since the jumps in the dataset occur at fixed intervals (`val = 3` timesteps) but are shifted between different time series, the model must capture the relationships between events based on their distances, which is effectively handled by relative positional encodings. Absolute positional encodings, on the other hand, can't capture these relative relationships, leading to poorer model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
